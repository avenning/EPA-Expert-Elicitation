---
title: "Data Development Procedure"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(readr)
library(lubridate)
library(MASS)
Raw_Data <- read_csv("Raw_Data_Head.csv")    # Replace with long form (remove "_Head")
MacroData <- read_csv("MacroData.csv")
BI_Data <- read_csv("BI_Data.csv")
Water_Quality <- read_csv("Water_Quality_Data.csv", 
                          col_types = cols(Day = col_double(),
                                           Unit = col_character()))
```

### Sources and Structure of Raw Data

Two data sources were used as the basis for generating sample ecological data to be used in the expert elicitation process. Both datasets were generated by the North Carolina Department of Environmental Quality (DEQ). The first source is a data set that includes a wide range of ecological measurements across 68 creeks and rivers within the Haw River and Upper Neuse River watersheds, and was used as the source of measurements for five biophysical metric variables: fecal coliform, specific conductance, total nitrogen, total phosphorus, and turbidity.

```{r raw_data_glimpse, echo = FALSE}
glimpse(Raw_Data)
```

The second data source is a set of macroinvertibrate data collected from creeks and rivers in the Upper Neuse River watershed and the Cape Fear River watershed, of which the Haw River is a major tributary. This data set served as source of biotic index measurement data.

```{r macro_data_glimpse, echo = FALSE}
glimpse(MacroData)
```

### Data Cleaning and Formatting

The first water quality dataset was processed to specify the name of the creek or river and the watershed in which it runs. Measurement type was filtered to include only six types: fecal coliform, inorganic nitrogen, Kjeldahl (organic) nitrogen, specific conductance, total phosphorus, and turbidity, and unit conversions were done to ensure all measurements in each category had the same unit. Measurments noted as being below the analytical threshold for that technique were included as zero values. The data were also filtered by date so that all measurements included were taken during the growing season, which in North Carolina occurs between March and November.  
  
The macroinvertibrate data were filtered to include only the measurements of biotic index along with geographic and temporal identification data. These data were then formatted to match the other ecological measurement data and the two datasets were merged. A small portion of this formatted data can be seen below.

```{r processed_data, echo = FALSE}
sample_n(Water_Quality, size = nrow(Water_Quality)) %>%
  dplyr::select(-Lat, -Long)
```

In order to determine the relationships between each of the water quality variables, the data were organized and formatted by determining each instance where all six measurements were taken in the same location on the same day. These events are idenified by creek name, date, and coordinates of the testing site. Once these events are identified, the water quality dataset is filtered to include only data collected from them. These data are organized so that each row represents a measurement event with six columns, one for each nutrient measurement.

```{r}
# Identify each instance where all six measurements were taken in a new column by concatenating the creek name, the date the measurement was taken, and the coordinates of the testing site.

WQ_wCDLL <- Water_Quality %>%
  mutate(CreekName = Creek,
         Date = mdy(paste(Month, Day, Year, sep = "/")),
         Date2 = Date, Lat2 = Lat, Long2 = Long) %>%
  unite(col = "LatLong", Lat2, Long2, sep = ", ") %>%
  unite(col = "Creek_Date_LatLong", CreekName, Date2, LatLong, sep = "_")

CDLL <- WQ_wCDLL %>%
  group_by(Creek_Date_LatLong, Measurement) %>%
  summarise(N = n()) %>%
  filter(Measurement != "Biotic Index (BI)") %>%
  spread(value = N, key = Measurement) %>%
  rename("FC" = `Fecal Coliform`,
         "IN" = `Inorganic Nitrogen`,
         "KN" = `Kjeldahl Nitrogen`,
         "TP" = `Total Phosphorus`,
         "SC" = `Specific Conductance`,
         "TU" = `Turbidity`) %>%
  filter(FC+IN+KN+TP+SC+TU == 6) %>%
  dplyr::select(Creek_Date_LatLong)

# Filter data to include only instances where all six ecological markers were measured in the same location on the same day. Spread data so that each ecological marker is it's own column and each observation includes all six measurements. Also create total nitrogen (TN) column by adding Kjeldahl and Inorganic Nitrogen columns together.

WQ <- WQ_wCDLL %>%
  filter(Creek_Date_LatLong %in% CDLL$Creek_Date_LatLong) %>%
  arrange(Creek_Date_LatLong) %>%
  dplyr::select(-Unit, -Month, -Day, -Year) %>%
  spread(key = Measurement, value = Value) %>%
  rename("FC" = `Fecal Coliform`,
         "IN" = `Inorganic Nitrogen`,
         "KN" = `Kjeldahl Nitrogen`,
         "TP" = `Total Phosphorus`,
         "SC" = `Specific Conductance`,
         "TU" = `Turbidity`) %>%
  mutate(TN = IN + KN) %>%
  dplyr::select(Creek, Date, FC, SC, IN, KN, TN, TP, TU, Lat, Long)

```

### Analysis of Ecological Data

The nitrogen measurements in the dataset skew much higher than expected for typical waterways in the included watersheds. This is likely due to an overrepresentation of testing sites downstream of wastewater treatment plants. A K-means cluster analysis was performed to identify testing events that are likely effected by wastewater treatment plant discharge. The water quality dataset was filtered to exclude these suspected outliers.

```{r}
# Cluster Analysis to exclude total nitrogen outliers likely caused by measurements downstream of waste-water treatment plants.

# K-Means Cluster Analysis
kWQ <- WQ$TN              # Narrow dataset to total nitrogen measurements
fit <- kmeans(kWQ, 2)     # Create 2 clusters using K-means

# Append cluster assignment to dataframe
WQ_Cluster <- data.frame(WQ, fit$cluster) %>%
  filter(fit.cluster == 1)
# Display mean total nitrogen values for each cluster group
aggregate(kWQ,by=list(fit$cluster),FUN=mean)

```

The fecal coliform data featured several outliers with extremely high values (Z-value > 45) which have an outsize influence on summary statistics and on the sample data generation process. A similar k-means cluster analysis was performed, and suspected outliers were removed from the dataset.

```{r}
# Cluster Analysis of fecal coliform to exclude outliers
WQ_Cluster %>%
  dplyr::select(FC) %>%
  summarise(Med = median(FC),                # Summarize fecal coliform data quantiles to demonstrate extreme skew of the data.
            Q75 = quantile(FC, 0.75),
            Q90 = quantile(FC, 0.90),
            Q95 = quantile(FC, 0.95),
            Q99 = quantile(FC, 0.99),
            Max = max(FC),
            Avg = round(mean(FC), digits = 2))

fWQ <- WQ_Cluster$FC
fitFC <- kmeans(fWQ, 2)

# Append cluster assignment to dataframe
FC_Cluster <- data.frame(WQ_Cluster, fitFC$cluster)
# Display mean fecal coliform values for each cluster
aggregate(fWQ,by=list(fitFC$cluster),FUN=mean)
```

Biotic index (BI) data are added to the dataset; however BI measurements were not made during the same measurement events and at the same sites as the nutrient values were collected. BI measurements are much more consistent for a particular stream than nutrient levels. Average BI measurements were calculated for each stream, and are joined to each row of nutrient data by stream.

```{r}
# Filter biotic index data to exclude creeks from outside the target watersheds. Calculate average BI data for each creek.
BI_Join <- BI_Data %>%
  filter(Creek %in% unique(WQ_Cluster$Creek)) %>%
  group_by(Creek) %>%
  summarise(BI = round(mean(Value), digits = 2))

# Join BI data to nutrient measurements such that each set of five nutrient measurements is paired with the average BI value for the particular stream it describes. (BI is much less variable than nutrient measurements for a given stream.)
WQ_Final <- WQ_Cluster %>%
  left_join(BI_Join, by = "Creek") %>%      
  dplyr::select(Creek, BI, FC, SC, TN, TP, TU) %>%
  filter(FC < 7200)    # Filter to remove fecal coliform outliers
```

### Sample Data Generation

Data are prepared for the generation algorithm by eliminating zero values so that the logorithm of the dataset can be taken, since the distribution is assumed to be log-normal. The logorithm of the dataset is taken to normalize the distribution, a sample dataset with 1000 rows is generated, and the data is converted back to the original log-normal distribution.

```{r}
# Prepare data for data generation algorithm. Zero values for nutrient measurements will create NA values when the logorithm is taken. The minimum non-zero value for each measurement is taken as the minimum detection threshold, and all zero values are replaced with this value.

WQ_Final$FC[WQ_Final$FC == 0] <- 1
WQ_Final$TP[WQ_Final$TP == 0] <- 0.01
WQ_Final$TU[WQ_Final$TU == 0] <- 0.1

# Create vectors and matrices to enable sample data generation algorithm. Normalize lognormal data before generating sample data - then return to lognormal format

Column_Means <- as.numeric(colMeans(WQ_Final[,2:7], na.rm = TRUE))

LogNorm <- WQ_Final[,2:7]
Norm <- log(LogNorm)

NormCov <- cov(Norm, use = "pairwise.complete.obs")
NormMeans <- log(Column_Means)

NormSample <- mvrnorm(n = 1000, mu = NormMeans, Sigma = NormCov)
LogNormSample <- exp(NormSample)
Sample_Data <- round(LogNormSample, 3)

```
  
```{r, echo = FALSE}
Sample_Data <- tibble(BI = as.numeric(t(Sample_Data[,1])),
                      FC = as.numeric(t(Sample_Data[,2])),
                      SC = as.numeric(t(Sample_Data[,3])),
                      TN = as.numeric(t(Sample_Data[,4])),
                      TP = as.numeric(t(Sample_Data[,5])),
                      TU = as.numeric(t(Sample_Data[,6])))

head(Sample_Data, n = 5)

Sample_Means <- colMeans(Sample_Data)
Sample_SDs <- Sample_Data %>%
  summarise(BI = sd(BI), FC = sd(FC), SC = sd(SC), 
            TN = sd(TN), TP = sd(TP), TU = sd(TU)) %>%
  as.numeric()

# Create a dataframe of the Z-value for each datum in the generated sample data.
ZVal <- tibble(Row = c(1:1000),
               BI = (Sample_Data$BI - Sample_Means[1]) / Sample_SDs[1],
               FC = (Sample_Data$FC - Sample_Means[2]) / Sample_SDs[2],
               SC = (Sample_Data$SC - Sample_Means[3]) / Sample_SDs[3],
               TN = (Sample_Data$TN - Sample_Means[4]) / Sample_SDs[4],
               TP = (Sample_Data$TP - Sample_Means[5]) / Sample_SDs[5],
               TU = (Sample_Data$TU - Sample_Means[6]) / Sample_SDs[6])
```

### Selection of Sample Data Rows

Z-Values are calculated for each ecological data point in the generated sample data. For each variable, the 50 data rows with greatest magnitude Z-value are selected. These rows are narrowed by excluding any row which features in the top 50 Z-values for more than one variable, and then 12 rows are selected at random for each variable, making up the first 72 data rows.

```{r}
T50 <- function(ColNum, ColName, Meas) {
  tibble(Row = ZVal[[1]],
         Z = abs(ZVal[[ColNum]])) %>%
  arrange(desc(Z)) %>%
  head(n = 50) %>%
  mutate(Meas = Meas)
}
```
```{r, echo = FALSE}
Top50 <- rbind(T50(ColNum = 2, ColName = BI, Meas = "BI"),
               T50(ColNum = 3, ColName = FC, Meas = "FC"),
               T50(ColNum = 4, ColName = SC, Meas = "SC"),
               T50(ColNum = 5, ColName = TN, Meas = "TN"),
               T50(ColNum = 6, ColName = TP, Meas = "TP"),
               T50(ColNum = 7, ColName = TU, Meas = "TU"))
```
```{r}
UniqueRows <- Top50 %>%
  group_by(Row) %>%
  summarise(N = n()) %>%
  filter(N == 1)

RowSelect <- Top50 %>%
  filter(Row %in% UniqueRows$Row) %>%
  group_by(Meas) %>%
  sample_n(12)
```

The remaining rows are selected by filtering the data to include only the lowest 80 percent of measurements for each water quality variable, and randomly selecting 40 observations from this data set. The 72 data rows representing the extremes of each variable and the 40 data rows that represent the more normal range of each variable are combined to form the final set of 112 generated data rows.

```{r}
Pct80 <- Sample_Data %>%
  gather(value = Value, key = Measurement) %>%
  group_by(Measurement) %>%
  summarise(Pct80 = quantile(Value, 0.8)) %>%
  dplyr::select(Pct80) %>%
  t() %>%                                              # Determine the 80th percentile for each variable;
  as.numeric()                                         # store these values in a vector: Pct80

Sample100 <- rbind(
  Sample_Data %>%                                      # Filter sample data to include selected rows from
    mutate(Row = 1:1000) %>%                           # top five percent of Z-values for each variable
    filter(Row %in% RowSelect$Row) %>%
    left_join(RowSelect, by = "Row") %>%
    dplyr::select(BI, FC, SC, TN, TP, TU, Meas, Z),
  
  Sample_Data %>%                                      # Filter the sample data to exclude the top
    filter(BI < Pct80[1],                              # 20 percent for each variable.
           FC < Pct80[2],
           SC < Pct80[3],
           TN < Pct80[4],
           TP < Pct80[5],
           TU < Pct80[6]) %>%
    sample_n(40) %>%
    mutate(Meas = NA, Z = NA)
  ) %>%
  arrange(Meas, desc(Z))

colnames(Sample100) <- c("Biotic Index", "Fecal Coliform (cfu/100mL)", 
                         "Specific Conductance (uS/cm)", 
                         "Total Nitrogen (mg/L)", "Total Phosphorus (mg/L)", 
                         "Turbidity (NTU)", "Key Variable", 
                         "Z-Value of Key Variable")

glimpse(Sample100)
```

The "Key Variable" column indicates which variable in the data row was among the top 50 in terms of the magnitude of the Z-value for that variable. Rows that were selected from the bottom 80 percent portion do not have a key variable. This is for visual organization and inspection of the data, and is not indicated to experts completing the elicitation.


